/// OpenAI/OpenRouter Prompt Caching Analysis Example
///
/// **Usage:**
/// ```bash
/// # Test OpenAI directly (format: provider:model)
/// MODELS="openai:gpt-5-mini" \
///   OPENAI_API_KEY="sk-..." \
///   cargo run --example 10_cache_analysis
///
/// # Test OpenAI via OpenRouter
/// MODELS="openrouter:openai/gpt-5-mini" \
///   OPENROUTER_API_KEY="sk-or-..." \
///   cargo run --example 10_cache_analysis
///
/// # Test multiple models (comma-separated)
/// MODELS="openai:gpt-5-mini,openrouter:google/gemini-2.5-flash-lite" \
///   OPENAI_API_KEY="sk-..." \
///   OPENROUTER_API_KEY="sk-or-..." \
///   cargo run --example 10_cache_analysis
///
/// # Test default FREE models (no provider prefix = openrouter)
/// MODELS="deepseek/deepseek-r1-0528:free,z-ai/glm-4.5-air:free" \
///   OPENROUTER_API_KEY="sk-or-..." \
///   cargo run --example 10_cache_analysis
/// ```

use buildscale::load_config;
use futures::StreamExt;
use rig::providers::openai;
use rig::client::CompletionClient;
use rig::streaming::StreamingChat;
use secrecy::{ExposeSecret, SecretString};
use std::time::{Duration, Instant};
use std::env;

#[derive(Debug, Clone)]
struct CacheTestConfig {
    name: String,
    model: String,
    base_url: Option<String>,
    provider: String,
    persona_size: usize,
}

#[derive(Debug)]
struct RequestResult {
    scenario: String,
    request_num: usize,
    latency_ms: u128,
    input_tokens: Option<u64>,
    output_tokens: Option<u64>,
    cached_tokens: Option<u64>,
    #[allow(dead_code)]
    text: String,
}

fn print_header(title: &str) {
    println!("\n{}", "=".repeat(80));
    println!("{}", title);
    println!("{}", "=".repeat(80));
}

fn print_result_row(result: &RequestResult) {
    let cache_hit = if let Some(cached) = result.cached_tokens {
        format!("‚úÖ HIT ({} cached)", cached)
    } else if result.request_num == 1 {
        "MISS (first)".to_string()
    } else if result.latency_ms < 500 {
        "HIT? (fast)".to_string()
    } else {
        "MAYBE (slow)".to_string()
    };

    println!(
        "  Req #{:<3} | {:>6}ms | In: {:>5} | Out: {:>4} | {}",
        result.request_num,
        result.latency_ms,
        result.input_tokens.unwrap_or(0),
        result.output_tokens.unwrap_or(0),
        cache_hit
    );
}

fn build_persona(lines: usize) -> String {
    let mut persona = r#"
You are an expert software architect and code reviewer.

Your responsibilities include:
1. Analyzing code structure and design patterns
2. Identifying potential bugs and security issues
3. Suggesting performance optimizations
4. Reviewing code quality and maintainability

You have access to the following tools:
- ls: List files in a directory
- read: Read file contents
- grep: Search for patterns in files
- write: Create or overwrite files
- edit: Make precise edits to files

When working with code, always:
- Start by understanding the current structure
- Make incremental changes
- Test after each modification
- Document your changes clearly
"#.to_string();

    let base_lines = persona.lines().count();
    for i in base_lines..lines {
        persona.push_str(&format!("\n[Line {}] This is additional context to increase the persona size and ensure we exceed the 1024-token threshold required for prompt caching to activate.\n", i));
    }

    persona
}

async fn run_scenario(
    config: &CacheTestConfig,
    api_key: &str,
    num_requests: usize,
    delays: &[Duration],
) -> Result<Vec<RequestResult>, Box<dyn std::error::Error>> {
    let mut results = Vec::new();

    print_header(&format!("MODEL: {}", config.name));
    println!("Provider: {} | Model: {} | Persona size: {} lines", config.provider, config.model, config.persona_size);
    if let Some(ref base_url) = config.base_url {
        println!("Base URL: {}", base_url);
    }
    println!("Running {} requests with delays: {:?}", num_requests, delays);
    println!();

    let persona = build_persona(config.persona_size);
    println!("Persona size: {} characters (~{} estimated tokens)", persona.len(), persona.len() / 4);

    let secret_key = SecretString::new(api_key.to_string().into());

    let client: openai::Client = if let Some(ref base_url) = config.base_url {
        openai::Client::builder()
            .api_key(secret_key.expose_secret())
            .base_url(base_url)
            .build()
            .map_err(|e| format!("Failed to create client with base_url: {}", e))?
    } else {
        openai::Client::new(secret_key.expose_secret())
            .map_err(|e| format!("Failed to create client: {}", e))?
    };

    let agent = client.agent(&config.model).preamble(&persona).build();

    for i in 0..num_requests {
        let request_num = i + 1;
        let delay = delays.get(i).map(|d| d.as_secs()).unwrap_or(0);

        if delay > 0 {
            println!("‚è≥ Waiting {} seconds before request #{}", delay, request_num);
            tokio::time::sleep(Duration::from_secs(delay)).await;
        }

        let prompt = format!("What is 2 + {}? Answer with just the number.", request_num);

        println!("üîÑ Request #{}: \"{}\"", request_num, prompt);
        let start = Instant::now();

        let mut stream = agent.stream_chat(&prompt, vec![]).await;
        let mut response_text = String::new();
        let mut final_usage = None;

        while let Some(item_result) = stream.next().await {
            match item_result {
                Ok(item) => {
                    match item {
                        rig::agent::MultiTurnStreamItem::StreamAssistantItem(content) => {
                            if let rig::streaming::StreamedAssistantContent::Text(text) = content {
                                response_text.push_str(&text.text);
                            }
                        }
                        rig::agent::MultiTurnStreamItem::FinalResponse(final_response) => {
                            final_usage = Some(final_response.usage());
                        }
                        _ => {}
                    }
                }
                Err(e) => {
                    eprintln!("Stream error: {}", e);
                    break;
                }
            }
        }

        let latency = start.elapsed().as_millis();

        // Extract token info from usage
        let (input_tokens, output_tokens, _total_tokens, cached_tokens) = if let Some(usage) = final_usage {
            if let Ok(json) = serde_json::to_value(&usage) {
                let input = json.get("input_tokens").or_else(|| json.get("prompt_tokens")).and_then(|v| v.as_u64());
                let output = json.get("output_tokens").or_else(|| json.get("completion_tokens")).and_then(|v| v.as_u64());
                let total = json.get("total_tokens").and_then(|v| v.as_u64());
                let cached = json.get("cached_tokens")
                    .or_else(|| json.get("prompt_tokens_details").and_then(|d| d.get("cached_tokens")))
                    .and_then(|v| v.as_u64());

                // Check for prompt_tokens_details structure
                if let Some(details) = json.get("prompt_tokens_details") {
                    println!("      ‚úÖ prompt_tokens_details: {}", serde_json::to_string_pretty(details).unwrap_or_else(|_| "N/A".to_string()));
                }

                (input, output, total, cached)
            } else {
                (None, None, None, None)
            }
        } else {
            (None, None, None, None)
        };

        // Print raw usage for debugging
        if let Some(usage) = final_usage {
            if let Ok(json) = serde_json::to_string_pretty(&usage) {
                if json.len() < 500 {
                    println!("      Usage: {}", json);
                }
            }
        }

        let result = RequestResult {
            scenario: config.name.clone(),
            request_num,
            latency_ms: latency,
            input_tokens,
            output_tokens,
            cached_tokens,
            text: response_text.clone(),
        };

        print_result_row(&result);
        results.push(result);

        let preview = response_text.trim().chars().take(80).collect::<String>();
        println!("      Response: \"{}{}\"", preview, if response_text.len() > 80 { "..." } else { "" });

        println!();
    }

    Ok(results)
}

fn analyze_results(all_results: &[Vec<RequestResult>]) {
    print_header("CACHE ANALYSIS SUMMARY - ALL MODELS");

    for scenario_results in all_results {
        if scenario_results.is_empty() {
            continue;
        }

        let scenario_name = &scenario_results[0].scenario;
        println!("\nüìä {}", scenario_name);

        let successful_results: Vec<_> = scenario_results.iter().filter(|r| r.latency_ms > 0).collect();

        if successful_results.is_empty() {
            println!("  ‚ö†Ô∏è  No successful requests");
            continue;
        }

        let avg_latency: u128 = successful_results.iter().map(|r| r.latency_ms).sum::<u128>() / successful_results.len() as u128;
        let min_latency = successful_results.iter().map(|r| r.latency_ms).min().unwrap_or(0);
        let max_latency = successful_results.iter().map(|r| r.latency_ms).max().unwrap_or(0);

        let has_cached_tokens = successful_results.iter().any(|r| r.cached_tokens.is_some());

        println!("  Requests: {}/{}", successful_results.len(), scenario_results.len());
        println!("  Latency: min={}ms, max={}ms, avg={}ms", min_latency, max_latency, avg_latency);

        if has_cached_tokens {
            println!("  ‚úÖ cached_tokens field: PRESENT");
            for r in &successful_results {
                if let Some(cached) = r.cached_tokens {
                    println!("     Req #{}: {} cached tokens", r.request_num, cached);
                }
            }
        } else {
            println!("  ‚ùå cached_tokens field: NOT FOUND (Rig v0.29 limitation)");
        }

        // Check if input_tokens are stable (potential caching indicator)
        let input_tokens: Vec<_> = successful_results.iter()
            .filter_map(|r| r.input_tokens)
            .collect::<Vec<_>>();

        if !input_tokens.is_empty() {
            let all_same = input_tokens.windows(2).all(|w| w[0] == w[1]);
            if all_same {
                println!("  üîí Input tokens stable: {} (infrastructure caching likely working)", input_tokens[0]);
            } else {
                println!("  üìä Input tokens vary: {:?} (no caching evidence)", input_tokens);
            }
        }
    }

    println!("\n{}", "=".repeat(80));
}

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    print_header("OpenAI/OpenRouter Prompt Caching Analysis");

    // Parse models from env: "provider:model" or "model" (defaults to openrouter)
    let models = if let Ok(models_env) = env::var("MODELS") {
        println!("Models from env: {}", models_env);
        models_env
            .split(',')
            .filter_map(|s| {
                let parts: Vec<&str> = s.trim().splitn(2, ':').collect();
                if parts.len() == 2 {
                    // Format: "provider:model"
                    let (provider, model) = (parts[0].trim(), parts[1].trim());
                    Some((provider.to_string(), model.to_string()))
                } else if parts.len() == 1 && !parts[0].is_empty() {
                    // Format: "model" (default to openrouter)
                    let model = parts[0].trim();
                    Some(("openrouter".to_string(), model.to_string()))
                } else {
                    None
                }
            })
            .collect::<Vec<_>>()
    } else {
        println!("Using default FREE models");
        vec![
            ("openrouter".to_string(), "deepseek/deepseek-r1-0528:free".to_string()),
            ("openrouter".to_string(), "z-ai/glm-4.5-air:free".to_string()),
            ("openrouter".to_string(), "google/gemma-3-27b-it:free".to_string()),
            ("openrouter".to_string(), "arcee-ai/trinity-mini:free".to_string()),
        ]
    };

    println!("Testing {} models:", models.len());
    for (provider, model) in &models {
        println!("  - {}:{} (provider: {})", provider, model, provider);
    }
    println!();

    let num_requests = 3;
    let delays = vec![
        Duration::from_secs(0),
        Duration::from_secs(2),
        Duration::from_secs(5),
    ];

    let mut all_results = Vec::new();

    for (provider, model_id) in models {
        // Get API key based on provider
        // Try provider-specific key first, then fall back to config
        let api_key = match provider.as_str() {
            "openai" => env::var("OPENAI_API_KEY")
                .or_else(|_| env::var("OPENROUTER_API_KEY"))
                .ok(),
            "openrouter" => env::var("OPENROUTER_API_KEY").ok(),
            _ => None,
        };

        let api_key = match api_key {
            Some(key) if !key.is_empty() => key,
            _ => {
                // Try to load from config
                let app_config = load_config().ok();
                match provider.as_str() {
                    "openai" => {
                        app_config
                            .and_then(|c| c.ai.providers.openai)
                            .map(|o| o.api_key.expose_secret().to_string())
                            .unwrap_or_else(|| {
                                // Fallback to openrouter config
                                load_config().ok()
                                    .and_then(|c| c.ai.providers.openrouter)
                                    .map(|o| o.api_key.expose_secret().to_string())
                                    .unwrap_or_else(|| "sk-test-key".to_string())
                            })
                    }
                    "openrouter" => app_config
                        .and_then(|c| c.ai.providers.openrouter)
                        .map(|o| o.api_key.expose_secret().to_string())
                        .unwrap_or_else(|| "sk-or-test-key".to_string()),
                    _ => "sk-test-key".to_string(),
                }
            }
        };

        println!("API key for {}: {}{}...", provider, &api_key[..4.min(api_key.len())], if api_key.len() > 4 { "..." } else { "" });

        // Set base_url based on provider
        let base_url = match provider.as_str() {
            "openai" => None,
            "openrouter" => Some("https://openrouter.ai/api/v1".to_string()),
            _ => Some("https://openrouter.ai/api/v1".to_string()),
        };

        let config = CacheTestConfig {
            name: format!("{}:{}", provider, model_id),
            model: model_id,
            base_url,
            provider,
            persona_size: 500,
        };

        match run_scenario(&config, &api_key, num_requests, &delays).await {
            Ok(results) => {
                all_results.push(results);
            }
            Err(e) => {
                eprintln!("‚ùå Model '{}' failed: {}", config.name, e);
            }
        }

        // Wait between models to avoid rate limiting
        tokio::time::sleep(Duration::from_secs(3)).await;
    }

    analyze_results(&all_results);

    println!("\n‚úÖ Analysis complete!");
    println!("\nüìù KEY FINDINGS:");
    println!("   - cached_tokens field: Rig v0.29 does NOT expose this from OpenAI/OpenRouter");
    println!("   - Stable input_tokens suggest infrastructure caching is working");
    println!("   - To verify actual cached_tokens, check OpenAI Dashboard or upgrade Rig");

    Ok(())
}
